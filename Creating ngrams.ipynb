{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Capstone Project - Preprocessing (R).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwNoCwN5-2S6"
      },
      "source": [
        "# Word Prediction Project\r\n",
        "The following notebook is part of the Coursera Data Science Specialization's capstone project, where I clean and transform a large corpus of text data into n-grams on a Spark cluster using the Sparklyr package. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMR38x77-xdH"
      },
      "source": [
        "## Setting up environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlC9BvgHj6di"
      },
      "source": [
        "Installing and loading required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySoydKRhYiwa"
      },
      "source": [
        "install.packages(\"tidytext\")\r\n",
        "install.packages(\"sparklyr\")\r\n",
        "install.packages(\"lexicon\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuhluzkrYqmA"
      },
      "source": [
        "library(tidyverse)\r\n",
        "library(tidytext)\r\n",
        "library(sparklyr)\r\n",
        "library(lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uo3oEDl-rar"
      },
      "source": [
        "Installing spark and connecting to cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79Fp1oBEYGW_"
      },
      "source": [
        "spark_install(version = \"3.0.1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHWYAf1hYAE6"
      },
      "source": [
        "sc <- spark_connect(master = \"local\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7YYjCwm7w0v"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE2n7FFY97hu"
      },
      "source": [
        "Data available here: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip  \r\n",
        "*Note that data needs to be uploaded to session's storage first*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ1s5USRoqR4"
      },
      "source": [
        "Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2v82CV3aAH5"
      },
      "source": [
        "alltxt <- sdf_bind_rows(spark_read_text(sc, path =\"en_US.blogs.txt\"),\r\n",
        "                        spark_read_text(sc, path =\"en_US.news.txt\"),\r\n",
        "                        spark_read_text(sc, path =\"en_US.twitter.txt\")) %>%\r\n",
        "  sdf_with_sequential_id(id = \"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zybz09-J-A6s"
      },
      "source": [
        "Cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpVzNkeHVE2B"
      },
      "source": [
        "profanity_regex <- paste0(lexicon::profanity_banned, collapse=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49S9lOyXQCo9"
      },
      "source": [
        "cleanedtxt <- alltxt %>%\r\n",
        "  mutate(line=regexp_replace(line, \"''|â€™\", \"'\")) %>% # Normalizing apostrophes\r\n",
        "  mutate(line=regexp_replace(line,\"[^a-zA-Z' ]\",\" _ \")) %>% # Create a placeholder for empty character so they can be filtered out later\r\n",
        "  mutate(line=regexp_replace(line,\" '|' |^'|'$\",\" _ \")) %>% # Remove apostrophes at the beginning or end of a word \r\n",
        "  mutate(line=regexp_replace(line,\"  \",\" \")) %>% # Removing white spaces that might have been created\r\n",
        "  mutate(line=tolower(line)) %>%\r\n",
        "  mutate(line=regexp_replace(line,profanity_regex, \"_\")) %>% # Removing profanities\r\n",
        "  select(id, line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nue4ijYt4UBw"
      },
      "source": [
        "cleanedtxt %>%\r\n",
        "  sdf_repartition(1) %>%\r\n",
        "  spark_write_csv(\"./cleaned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4cEL0dGX18i"
      },
      "source": [
        "## Creating n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-3OpEq4cLNK"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6jRY2hZW3Zc"
      },
      "source": [
        "toks <- cleanedtxt %>%\r\n",
        "  ft_tokenizer(input_col=\"line\", output_col=\"tokens\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8GGqLvd7qc"
      },
      "source": [
        "Unigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gapa9Elvd6jp"
      },
      "source": [
        "unigrams <- toks %>%\r\n",
        "  mutate(ngrams=explode(tokens)) %>%\r\n",
        "  filter(!grepl(\"_\",ngrams)) %>% # Removing placeholders created earlier\r\n",
        "  group_by(ngrams) %>%\r\n",
        "  summarise(n=n()) %>%\r\n",
        "  filter(n>5) %>%\r\n",
        "  arrange(desc(n))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjGWvZX8elKQ"
      },
      "source": [
        "unigrams %>%\r\n",
        "  sdf_repartition(1) %>%\r\n",
        "  spark_write_csv(\"./unigram2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6OajkPXcNM7"
      },
      "source": [
        "Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVx_h3qSX5Q5"
      },
      "source": [
        "bigrams <- toks %>%\r\n",
        "  ft_ngram(input_col = \"tokens\", output_col = \"words\", n=2) %>%\r\n",
        "  mutate(ngrams=explode(words)) %>%\r\n",
        "  filter(!grepl(\"_\",ngrams)) %>% \r\n",
        "  ft_regex_tokenizer(input_col=\"ngrams\", output_col=\"split\", pattern=\" \") %>% \r\n",
        "  sdf_separate_column(\"split\", into=c(\"word1\", \"word2\")) %>%\r\n",
        "  group_by(word1, word2) %>%\r\n",
        "  summarise(n=n()) %>%\r\n",
        "  filter(n>5) %>%\r\n",
        "  arrange(desc(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Ha0cVcX4xA"
      },
      "source": [
        "bigrams %>%\r\n",
        "  sdf_repartition(1) %>%\r\n",
        "  spark_write_csv(\"./bigrams3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7k2xQghdvtc"
      },
      "source": [
        "Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm80i94MY1Jx"
      },
      "source": [
        "trigrams <- toks %>%\r\n",
        "  ft_ngram(input_col = \"tokens\", output_col = \"words\", n=3) %>%\r\n",
        "  mutate(ngrams=explode(words)) %>%\r\n",
        "  filter(!grepl(\"_\",ngrams)) %>%\r\n",
        "  ft_regex_tokenizer(input_col=\"ngrams\", output_col=\"split\", pattern=\" \") %>% \r\n",
        "  sdf_separate_column(\"split\", into=c(\"word1\", \"word2\",\"word3\")) %>%\r\n",
        "  mutate(word1_2 = paste(word1, word2)) %>%\r\n",
        "  group_by(word1_2, word3) %>%\r\n",
        "  summarise(n=n()) %>%\r\n",
        "  filter(n>5) %>%\r\n",
        "  arrange(desc(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUk5oKTBn_HQ"
      },
      "source": [
        "trigrams %>%\r\n",
        "  sdf_repartition(1) %>%\r\n",
        "  spark_write_csv(\"./trigrams2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw7j5Q5-ds_q"
      },
      "source": [
        "Fourgrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE_qRLt3oDbZ"
      },
      "source": [
        "fourgrams <- toks %>%\r\n",
        "  ft_ngram(input_col = \"tokens\", output_col = \"words\", n=4) %>%\r\n",
        "  mutate(ngrams=explode(words)) %>%\r\n",
        "  filter(!grepl(\"_\",ngrams)) %>%\r\n",
        "  ft_regex_tokenizer(input_col=\"ngrams\", output_col=\"split\", pattern=\" \") %>% \r\n",
        "  sdf_separate_column(\"split\", into=c(\"word1\", \"word2\",\"word3\",\"word4\")) %>%\r\n",
        "  mutate(word1_3 = paste(word1, word2, word3)) %>%\r\n",
        "  group_by(word1_3, word4) %>%\r\n",
        "  summarise(n=n()) %>%\r\n",
        "  filter(n>5) %>%\r\n",
        "  arrange(desc(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQCPPaL_oDM2"
      },
      "source": [
        "fourgrams %>%\r\n",
        "  sdf_repartition(1) %>%\r\n",
        "  spark_write_csv(\"./fourgrams\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awnvofIpkK_n"
      },
      "source": [
        "Fivegrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3QEge3lBBru"
      },
      "source": [
        "fivegrams <- toks %>%\r\n",
        "  ft_ngram(input_col = \"tokens\", output_col = \"words\", n=5) %>%\r\n",
        "  mutate(ngrams=explode(words)) %>%\r\n",
        "  filter(!grepl(\"_\",ngrams)) %>%\r\n",
        "  ft_regex_tokenizer(input_col=\"ngrams\", output_col=\"split\", pattern=\" \") %>% \r\n",
        "  sdf_separate_column(\"split\", into=c(\"word1\", \"word2\",\"word3\",\"word4\",\"word5\")) %>%\r\n",
        "  mutate(word1_4 = paste(word1, word2, word3, word4)) %>%\r\n",
        "  group_by(word1_4, word5) %>%\r\n",
        "  summarise(n=n()) %>%\r\n",
        "  filter(n>5) %>%\r\n",
        "  arrange(desc(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbmeqhuKBBZT"
      },
      "source": [
        "fivegrams %>%\r\n",
        "  sdf_repartition(1) %>%\r\n",
        "  spark_write_csv(\"./fivegrams\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
