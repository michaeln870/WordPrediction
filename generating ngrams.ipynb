{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Creating n-grams with Sparklyr (R).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwNoCwN5-2S6"
      },
      "source": [
        "# Data Preparation: Generating N-grams\n",
        "In the following notebook, I use a large text corpus to generate N-grams on a Spark cluster using the Sparklyr package. This is part of the my capstone project from the Johns Hopkins University's Data Science Specialization on Coursera. The project aims at building a word prediction application.\n",
        "\n",
        "The code below:\n",
        "*   Cleans the data of all unwanted characters and breaks lines into sequences\n",
        "*   Generates n-grams from 1-grams to 5-grams with count. For 2-grams to 5-grams, these are then split into \"firstTerms\" and \"lastTerm\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMR38x77-xdH"
      },
      "source": [
        "## Setting up environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlC9BvgHj6di"
      },
      "source": [
        "Installing and loading required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySoydKRhYiwa"
      },
      "source": [
        "install.packages(\"sparklyr\")\n",
        "install.packages(\"lexicon\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuhluzkrYqmA"
      },
      "source": [
        "library(sparklyr)\n",
        "library(lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uo3oEDl-rar"
      },
      "source": [
        "Installing spark and connecting to cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79Fp1oBEYGW_"
      },
      "source": [
        "spark_install(version = \"3.0.1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHWYAf1hYAE6"
      },
      "source": [
        "sc <- spark_connect(master = \"local\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7YYjCwm7w0v"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE2n7FFY97hu"
      },
      "source": [
        "Data available [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).  \n",
        "*Note that data needs to be uploaded to session's storage first*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2v82CV3aAH5"
      },
      "source": [
        "alltxt <- sdf_bind_rows(spark_read_text(sc, path =\"en_US.blogs.txt\"),\n",
        "                        spark_read_text(sc, path =\"en_US.news.txt\"),\n",
        "                        spark_read_text(sc, path =\"en_US.twitter.txt\")) %>%\n",
        "  sdf_with_sequential_id(id = \"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGPhOnRXmmv_"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpVzNkeHVE2B"
      },
      "source": [
        "profanity_regex <- paste0(lexicon::profanity_banned, collapse=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49S9lOyXQCo9"
      },
      "source": [
        "cleanedtxt <- alltxt %>%\n",
        "  mutate(line=regexp_replace(line, \"''|’\", \"'\")) %>% # Normalizing apostrophes\n",
        "  mutate(line=regexp_replace(line, '“|”', '\"')) %>% #Normalizing quotes\n",
        "  mutate(line=regexp_replace(line, \"_\",\"#\")) %>% # Clearing the placeholder character\n",
        "  mutate(line=regexp_replace(line,\"[,.!?;:\\\"()]\",\" _ \")) %>% # Spacing words from characters sequence breaks\n",
        "  mutate(line=regexp_replace(line,\"^['-]|['-]$|(?<![a-zA-Z])['-]|['-](?![a-zA-Z])\",\" _ \")) %>% # Dealing with apostrophes and hyphens outside of words\n",
        "  mutate(line=regexp_replace(line,\"[^-a-zA-Z' ]\",\"_\")) %>% # Removing everything else, except letters, and apostrophes and hyphens inside a word\n",
        "  mutate(line=regexp_replace(line,\"  +\",\" \")) %>% # Removing white spaces between words\n",
        "  mutate(line=tolower(line)) %>%\n",
        "  mutate(line=regexp_replace(line,profanity_regex, \"_\")) %>% # Removing profanities\n",
        "  select(id, line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nue4ijYt4UBw"
      },
      "source": [
        "cleanedtxt %>%\n",
        "  sdf_repartition(1) %>%\n",
        "  spark_write_csv(\"./cleaned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4cEL0dGX18i"
      },
      "source": [
        "## Generating n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-3OpEq4cLNK"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6jRY2hZW3Zc"
      },
      "source": [
        "toks <- cleanedtxt %>%\n",
        "  ft_tokenizer(input_col=\"line\", output_col=\"tokens\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8GGqLvd7qc"
      },
      "source": [
        "Unigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gapa9Elvd6jp"
      },
      "source": [
        "unigrams <- toks %>%\n",
        "  mutate(ngrams=explode(tokens)) %>%\n",
        "  filter(!grepl(\"_\",ngrams)) %>% # Removing placeholders created earlier\n",
        "  group_by(ngrams) %>%\n",
        "  summarise(n=n()) %>%\n",
        "  filter(n>1) %>%\n",
        "  arrange(desc(n))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjGWvZX8elKQ"
      },
      "source": [
        "unigrams %>%\n",
        "  sdf_repartition(1) %>%\n",
        "  spark_write_csv(\"./unigrams\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6OajkPXcNM7"
      },
      "source": [
        "Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVx_h3qSX5Q5"
      },
      "source": [
        "bigrams <- toks %>%\n",
        "  ft_ngram(input_col = \"tokens\", output_col = \"words\", n=2) %>%\n",
        "  mutate(ngrams=explode(words)) %>%\n",
        "  filter(!grepl(\"_\",ngrams)) %>% \n",
        "  ft_regex_tokenizer(input_col=\"ngrams\", output_col=\"split\", pattern=\" \") %>% \n",
        "  sdf_separate_column(\"split\", into=c(\"firstTerms\", \"lastTerm\")) %>%\n",
        "  group_by(firstTerms, lastTerm) %>%\n",
        "  summarise(n=n()) %>%\n",
        "  filter(n>1) %>%\n",
        "  arrange(desc(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Ha0cVcX4xA"
      },
      "source": [
        "bigrams %>%\n",
        "  sdf_repartition(1) %>%\n",
        "  spark_write_csv(\"./bigrams\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7k2xQghdvtc"
      },
      "source": [
        "Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm80i94MY1Jx"
      },
      "source": [
        "trigrams <- toks %>%\n",
        "  ft_ngram(input_col = \"tokens\", output_col = \"words\", n=3) %>%\n",
        "  mutate(ngrams=explode(words)) %>%\n",
        "  filter(!grepl(\"_\",ngrams)) %>%\n",
        "  ft_regex_tokenizer(input_col=\"ngrams\", output_col=\"split\", pattern=\" \") %>% \n",
        "  sdf_separate_column(\"split\", into=c(\"word1\", \"word2\",\"lastTerm\")) %>%\n",
        "  mutate(firstTerms = paste(word1, word2)) %>%\n",
        "  group_by(firstTerms, lastTerm) %>%\n",
        "  summarise(n=n()) %>%\n",
        "  filter(n>1) %>%\n",
        "  arrange(desc(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUk5oKTBn_HQ"
      },
      "source": [
        "trigrams %>%\n",
        "  sdf_repartition(1) %>%\n",
        "  spark_write_csv(\"./trigrams\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw7j5Q5-ds_q"
      },
      "source": [
        "Fourgrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE_qRLt3oDbZ"
      },
      "source": [
        "fourgrams <- toks %>%\n",
        "  ft_ngram(input_col = \"tokens\", output_col = \"words\", n=4) %>%\n",
        "  mutate(ngrams=explode(words)) %>%\n",
        "  filter(!grepl(\"_\",ngrams)) %>%\n",
        "  ft_regex_tokenizer(input_col=\"ngrams\", output_col=\"split\", pattern=\" \") %>% \n",
        "  sdf_separate_column(\"split\", into=c(\"word1\", \"word2\",\"word3\",\"lastTerm\")) %>%\n",
        "  mutate(firstTerms = paste(word1, word2, word3)) %>%\n",
        "  group_by(firstTerms, lastTerm) %>%\n",
        "  summarise(n=n()) %>%\n",
        "  filter(n>1) %>%\n",
        "  arrange(desc(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQCPPaL_oDM2"
      },
      "source": [
        "fourgrams %>%\n",
        "  sdf_repartition(1) %>%\n",
        "  spark_write_csv(\"./fourgrams\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awnvofIpkK_n"
      },
      "source": [
        "Fivegrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3QEge3lBBru"
      },
      "source": [
        "fivegrams <- toks %>%\n",
        "  ft_ngram(input_col = \"tokens\", output_col = \"words\", n=5) %>%\n",
        "  mutate(ngrams=explode(words)) %>%\n",
        "  filter(!grepl(\"_\",ngrams)) %>%\n",
        "  ft_regex_tokenizer(input_col=\"ngrams\", output_col=\"split\", pattern=\" \") %>% \n",
        "  sdf_separate_column(\"split\", into=c(\"word1\", \"word2\",\"word3\",\"word4\",\"lastTerm\")) %>%\n",
        "  mutate(firstTerms = paste(word1, word2, word3, word4)) %>%\n",
        "  group_by(firstTerms, lastTerm) %>%\n",
        "  summarise(n=n()) %>%\n",
        "  filter(n>1) %>%\n",
        "  arrange(desc(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbmeqhuKBBZT"
      },
      "source": [
        "fivegrams %>%\n",
        "  sdf_repartition(1) %>%\n",
        "  spark_write_csv(\"./fivegrams\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}